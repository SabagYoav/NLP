{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoavvvvvv/NLP/blob/main/Whatsup_corpus_analysis_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the corpus"
      ],
      "metadata": {
        "id": "vOq7t5CCRx2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "V44-PcwUAQgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6leDhaO-JF3_",
        "outputId": "490420e9-1955-45a4-f852-982ea5cfe4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w3_WsNJRXBz"
      },
      "outputs": [],
      "source": [
        "# Step 1: Read the text file into copus\n",
        "with open('EN_chat.txt', 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_chat(chat):\n",
        "  cleaned_chat = [message.split(': ')[-1].split('\\\\')[0].strip('\\n') for message in chat]\n",
        "  cleaned_chat = cleaned_chat[2:]\n",
        "  return cleaned_chat"
      ],
      "metadata": {
        "id": "O3K_mBUN9bOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_lines = clean_chat(lines)\n",
        "for i in range(len(cleaned_lines)):\n",
        "  print(i, cleaned_lines[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OFA7vsX92z4",
        "outputId": "4f765b43-752e-4ce8-ad61-75c3cae01aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Hello my friends\n",
            "1 Hi everyone\n",
            "2 Oh, god üôà\n",
            "3 sup\n",
            "4 R U serious?\n",
            "5 Make sure you use good spelling haha ‚Äé<This message was edited>\n",
            "6 lol\n",
            "7 The purpose of this group is to create a nonsense chat\n",
            "8 With no sensitive info\n",
            "9 It was really warm today, has anybody noticed?\n",
            "10 Can haha be tokenized?\n",
            "11 I hope so üòÖ\n",
            "12 lets find out\n",
            "13 Yes. It‚Äôs spring time\n",
            "14 You can press on the auto complete and make some interesting endless sentences..\n",
            "15 ‚Äéimage omitted\n",
            "16 Oh‚Ä¶ it‚Äôs a good demonstration of NLP attention model\n",
            "17 Look what is the outcome :\n",
            "18 You are wrong with me now I am not feeling well and I am not feeling well and I am not feeling well\n",
            "19 Bibi netanyahu is the best player in the world and he is the best in the world but he is not the best in the world\n",
            "20 in Hebrew its better\n",
            "21 You can also try not the middle one every time, and create more diverse sentences..\n",
            "22 I can‚Äôt find that feature\n",
            "23 Like this :\n",
            "24 I talked to Yehuda and you said that you were in a relationship with the needed infrastructure and the best of the best to do it for you is the best thing ever ever ever ever do with you in the world and I have to do it with you all the time I have to be in my life with you all the way to your life and your life to you and your family to make you happy and blessed you to your family and God bless you and your heart 6 and your family 95 and\n",
            "25 amazing\n",
            "26 Is it WhatsApp feature?\n",
            "27 so guys do you have any planned trips? üõ¨\n",
            "28 Nope. It's the keyboard.\n",
            "29 üëÜüèº\n",
            "30 It's only on mobile\n",
            "31 Maybe to Lebanon üá±üáß\n",
            "32 Waze sometimes takes me there\n",
            "33 I want to plan my honeymoon for next year :)\n",
            "34 my waze takes me to Egypt\n",
            "35 great, where?\n",
            "36 You are genius\n",
            "37 I would like to travel to the ‚ÄúRamat Hagolan‚Äù‚Ä¶ it‚Äôs beautiful there.\n",
            "38 My mother in law is in Vietnam right now.. üõ´\n",
            "39 Mine take me to Beirut airport\n",
            "40 It‚Äôs risky to order tickets right now\n",
            "41 my work place want to send me to China in the middle of march\n",
            "42 Oh no! With all the projects and stuff?\n",
            "43 I‚Äôm planning to travel ti Panama with my family in Passover\n",
            "44 China is really cool!\n",
            "45 but I don't know how to eat with chopsticks\n",
            "46 It's always Beirut or Cairo.. üá±üáßüá™üá¨\n",
            "47 Hong Kong?\n",
            "48 wish to\n",
            "49 shenzen\n",
            "50 Yessss\n",
            "51 yeees\n",
            "52 Take a fork with you üòú\n",
            "53 in the pocket ü´£\n",
            "54 They might consider it a weapon before boarding in security control\n",
            "55 üòÇ\n",
            "56 How NLTK deal with emojis?\n",
            "57 that's why I think we should go with \"haha\" haha\n",
            "58 I don't think he saves emojis in the text\n",
            "59 lol\n",
            "60 Once, I accidentally took 2 scissors with me. It was inside my first aid kit. They found it on the scanning machine. They open the bag and took one of them out. They didn't check it again, so I ended up with one pair of scissors on the plane! ‚úÇÔ∏è ‚Äé<This message was edited>\n",
            "61 wooow\n",
            "62 such a crime\n",
            "63 I'm considering getting a dog, thoughts?\n",
            "64 take a cat\n",
            "65 ‚Äéimage omitted\n",
            "66 Cute! i like cats\n",
            "67 Their paws are addorable\n",
            "68 I'm allergic though\n",
            "69 üòî\n",
            "70 Especially hair balls ü§Æ\n",
            "71 I can understand why scissors are forbidden. But why they took my tweezers!\n",
            "72 I‚Äôm kidding. I like cats.\n",
            "73 ‚Äéimage omitted\n",
            "74 We need at least 50 messages\n",
            "75 Getting there..\n",
            "76 That is a very pretty picture\n",
            "77 Oh‚Ä¶ such a cute ü•∞\n",
            "78 ‚Äéimage omitted\n",
            "79 While she‚Äôs in Vietnam ?\n",
            "80 Exactly\n",
            "81 It‚Äôs payback for all the babysitting she made üòú\n",
            "82 She is in pension.. Having her best times of her life!\n",
            "83 Way to go!\n",
            "84 If she's not abroad, she's in Eilat... üèñÔ∏è\n",
            "85 How old is she?\n",
            "86 Guys, what is your favorite meal?\n",
            "87 Sorry. That was a personal question. ‚Äé<This message was edited>\n",
            "88 67. No problem ü§∑üèª‚Äç‚ôÇÔ∏è\n",
            "89 I love Italian food.\n",
            "90 Me too\n",
            "91 Carbs\n",
            "92 I like burgers, yum\n",
            "93 haha carbs\n",
            "94 Especially arenchini I don‚Äôt know how to say it in English\n",
            "95 Entrecote ü•©\n",
            "96 You make me hungry\n",
            "97 Like most men üòú\n",
            "98 \n",
            "99 Italian cuisine is one of the most popular and influential culinary traditions in the world, known for its emphasis on fresh, high-quality ingredients, regional diversity, and a balance of simplicity and flavor. It has evolved over centuries, influenced by various factors such as geography, economic conditions, and neighboring cultures.\n",
            "100 we just ate happy yummy carbs\n",
            "101 It was 2 hours ago\n",
            "102 you're right\n",
            "103 I thought at the beginning you wrote crabs‚Ä¶ ü¶Ä \n",
            "104 But it‚Äôs not kosher\n",
            "105 (But delicious, pardon me my religious friends)\n",
            "106 Do you think we need more data?\n",
            "107 I think it looks great..\n",
            "108 Cool\n",
            "109 I will export the chat and send to all our class\n",
            "110 Thank you for your cooperation üôè\n",
            "111 No, no. Thank YOU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Tokenize the corpus using:  NLTK's whitespace tokenizer"
      ],
      "metadata": {
        "id": "M7wZeMBxJm9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to activate nltk's white space tokenizer over the chat\n",
        "def nltk_white_space_tokenizer(text_list):\n",
        "  tk = WhitespaceTokenizer()\n",
        "  text_list_tokens = [tk.tokenize(text) for text in text_list]\n",
        "  return text_list_tokens"
      ],
      "metadata": {
        "id": "obDSD-na_IHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#activate the white space tokenizer\n",
        "nltk_whiteSpace_tokens = nltk_white_space_tokenizer(cleaned_lines)"
      ],
      "metadata": {
        "id": "9UZOauVdAyR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_whiteSpace_tokens[:3] #type of nltk_tokens is: list of list of str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENnTmp69CQP8",
        "outputId": "269286fb-1e5b-4541-9f42-80a27e472fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hello', 'my', 'friends'], ['Hi', 'everyone'], ['Oh,', 'god', 'üôà']]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Tokenize the corpus using: NLTK's sentence tokenizer\n"
      ],
      "metadata": {
        "id": "aFvjNLuBJrmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to activate sentance tokenizer over the chat\n",
        "def nltk_sent_tokenizer(text_list):\n",
        "  text_list_tokens = [sent_tokenize(text) for text in text_list]\n",
        "  return text_list_tokens"
      ],
      "metadata": {
        "id": "kAF0Zly1CRou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#activate the stentace tokenizer\n",
        "nltk_sent_tokens = nltk_sent_tokenizer(cleaned_lines)\n",
        "nltk_sent_tokens[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UryZG3-LIB2H",
        "outputId": "e22bc781-b904-46c5-8afd-c79b6cba775f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hello my friends'], ['Hi everyone'], ['Oh, god üôà']]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Tokenize the corpus using:  NLTK's word tokenizer\n"
      ],
      "metadata": {
        "id": "utAtIvA6KZz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to activate nltk's word tokenizer over the chat\n",
        "def nltk_word_tokenizer(text_list):\n",
        "  text_list_tokens = [word_tokenize(text) for text in text_list]\n",
        "  return text_list_tokens"
      ],
      "metadata": {
        "id": "rtYtRRVIJWce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_word_tokens = nltk_word_tokenizer(cleaned_lines)\n",
        "nltk_word_tokens[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMbS1bzEK1lG",
        "outputId": "7268e5f8-57ae-40a1-d413-86d3dac6e96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hello', 'my', 'friends'], ['Hi', 'everyone'], ['Oh', ',', 'god', 'üôà']]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Tokenize the corpus using:  NLTK's wordpunct tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "NcV4pUHMLkkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to activate nltk's wordpunct tokenizer over the chat\n",
        "def nltk_wordpunct_tokenizer(text_list):\n",
        "  text_list_wordpunct_tokens = [wordpunct_tokenize(text) for text in text_list]\n",
        "  return text_list_wordpunct_tokens"
      ],
      "metadata": {
        "id": "muely3nBLCrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_wordpunct_tokens = nltk_wordpunct_tokenizer(cleaned_lines)\n",
        "nltk_wordpunct_tokens[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ5usBsnLjrU",
        "outputId": "ab7a7882-ddd3-4397-8741-053d2d99f33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hello', 'my', 'friends'], ['Hi', 'everyone'], ['Oh', ',', 'god', 'üôà']]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Normalize the corpus by:  NLTK's Porter stemmer\n"
      ],
      "metadata": {
        "id": "DgB4qYQ5NPVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def stem_chat(text_list):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_chat = [[stemmer.stem(token) for token in tokens] for tokens in text_list]\n",
        "    return stemmed_chat"
      ],
      "metadata": {
        "id": "9z5YdPWpMliu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_tokens = nltk_white_space_tokenizer(cleaned_lines)\n",
        "stemmed_lines = stem_chat(lines_tokens)\n",
        "stemmed_lines[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhc2Q1oQOfpb",
        "outputId": "4c7da946-f822-41ef-828a-2cf66b29c0de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['hello', 'my', 'friend'], ['hi', 'everyon'], ['oh,', 'god', 'üôà']]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Normalize the corpus by:  NLTK's WordNet lemmatizer"
      ],
      "metadata": {
        "id": "AtlSUdsWNYc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ-3qwP5Qfxl",
        "outputId": "ecbe4ce9-ed04-40e8-b7d6-af5817f1ef14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "def lemmatize_chat(text_list):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_chat = []\n",
        "    for tokens in text_list:\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        lemmatized_chat.append(lemmatized_tokens)\n",
        "    return lemmatized_chat"
      ],
      "metadata": {
        "id": "4oDIZX9CNb2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_tokens = nltk_white_space_tokenizer(cleaned_lines)\n",
        "lemmatized_lines = lemmatize_chat(lines_tokens)\n",
        "lemmatized_lines[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTxkKgxJQCtW",
        "outputId": "1c2f786a-c747-4525-db35-2947d778521a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hello', 'my', 'friend'], ['Hi', 'everyone'], ['Oh,', 'god', 'üôà']]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Remove common English stop words"
      ],
      "metadata": {
        "id": "m42Ta3CYSzzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "fstOz95HQMVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9787d0e-1d5d-4d70-b74e-897cb6adee1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_stop_words(text_List):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_text=[]\n",
        "  for text in text_List:\n",
        "    filtered_words = [word for word in text if word.lower() not in stop_words]\n",
        "    filtered_text.append(filtered_words)\n",
        "  return filtered_text"
      ],
      "metadata": {
        "id": "XEzAT8BQTH3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_tokens = nltk_white_space_tokenizer(cleaned_lines)\n",
        "filterd_text  = clean_stop_words(lines_tokens)\n",
        "filterd_text[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6jfhQqXTo0y",
        "outputId": "98b0f760-e365-494f-8e59-fc6fd7320f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hello', 'friends'], ['Hi', 'everyone'], ['Oh,', 'god', 'üôà']]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Bag-of-words"
      ],
      "metadata": {
        "id": "_4jjQTYlebT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "9OBsnujGVKx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(texts):\n",
        "    vectorizer = CountVectorizer(max_features=1000)\n",
        "    features = vectorizer.fit_transform(texts)\n",
        "\n",
        "    return features, vectorizer"
      ],
      "metadata": {
        "id": "7n1P-vStbPq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_text_flat = [item for sublist in clean_stop_words(lines_tokens) for item in sublist]\n",
        "features_bow, vectorizer_bow = bag_of_words(filtered_text_flat)"
      ],
      "metadata": {
        "id": "yIXlE_8HW3Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of feature matrix:\", features_bow.shape)\n",
        "print(\"The space complexity of the algorithm is:\",features_bow.shape[0]*features_bow.shape[1],\" units\")"
      ],
      "metadata": {
        "id": "XVcF8PmpXDL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba65714-c243-4317-e2da-1f9a04b7b45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of feature matrix: (432, 288)\n",
            "The space complexity of the algorithm is: 124416  units\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer_bow.vocabulary_)"
      ],
      "metadata": {
        "id": "_1bd_q1FZ6_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbc88f3-89a1-4b8a-9945-51cbc88b76a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hello': 114, 'friends': 98, 'hi': 115, 'everyone': 81, 'oh': 177, 'god': 103, 'sup': 235, 'serious': 226, 'make': 151, 'sure': 236, 'use': 263, 'good': 104, 'spelling': 232, 'haha': 109, 'this': 246, 'message': 157, 'edited': 69, 'lol': 145, 'purpose': 207, 'group': 106, 'create': 53, 'nonsense': 173, 'chat': 40, 'sensitive': 224, 'info': 125, 'really': 211, 'warm': 267, 'today': 254, 'anybody': 14, 'noticed': 175, 'tokenized': 255, 'hope': 119, 'lets': 142, 'find': 90, 'yes': 284, 'it': 130, 'spring': 233, 'time': 252, 'press': 203, 'auto': 18, 'complete': 45, 'interesting': 129, 'endless': 75, 'sentences': 225, 'image': 122, 'omitted': 179, 'demonstration': 62, 'nlp': 170, 'attention': 17, 'model': 163, 'look': 146, 'outcome': 185, 'wrong': 279, 'feeling': 89, 'well': 271, 'bibi': 28, 'netanyahu': 168, 'best': 26, 'player': 200, 'world': 277, 'hebrew': 113, 'better': 27, 'also': 11, 'try': 260, 'middle': 159, 'one': 181, 'every': 80, 'diverse': 63, 'can': 35, 'feature': 88, 'like': 144, 'talked': 239, 'yehuda': 283, 'said': 217, 'relationship': 213, 'needed': 166, 'infrastructure': 126, 'thing': 244, 'ever': 79, 'life': 143, 'way': 268, 'family': 86, 'happy': 111, 'blessed': 30, 'bless': 29, 'heart': 112, '95': 2, 'amazing': 13, 'whatsapp': 272, 'guys': 107, 'planned': 198, 'trips': 259, 'nope': 174, 'keyboard': 132, 'mobile': 162, 'maybe': 153, 'lebanon': 141, 'waze': 269, 'sometimes': 230, 'takes': 238, 'want': 266, 'plan': 196, 'honeymoon': 117, 'next': 169, 'year': 281, 'egypt': 70, 'great': 105, 'where': 273, 'genius': 99, 'would': 278, 'travel': 258, 'ramat': 210, 'hagolan': 108, 'beautiful': 23, 'there': 243, 'mother': 164, 'law': 139, 'vietnam': 265, 'right': 215, 'now': 176, 'mine': 161, 'take': 237, 'beirut': 25, 'airport': 9, 'risky': 216, 'order': 183, 'tickets': 251, 'work': 276, 'place': 195, 'send': 223, 'china': 42, 'march': 152, 'no': 172, 'projects': 206, 'stuff': 234, 'planning': 199, 'ti': 250, 'panama': 187, 'passover': 189, 'cool': 50, 'know': 135, 'eat': 67, 'chopsticks': 43, 'always': 12, 'cairo': 34, 'hong': 118, 'kong': 137, 'wish': 274, 'shenzen': 228, 'yessss': 285, 'yeees': 282, 'fork': 95, 'pocket': 201, 'might': 160, 'consider': 47, 'weapon': 270, 'boarding': 31, 'security': 222, 'control': 49, 'nltk': 171, 'deal': 60, 'emojis': 72, 'that': 242, 'think': 245, 'go': 102, 'saves': 218, 'text': 240, 'once': 180, 'accidentally': 4, 'took': 256, 'scissors': 221, 'me': 154, 'inside': 128, 'first': 91, 'aid': 8, 'kit': 134, 'found': 96, 'scanning': 220, 'machine': 149, 'open': 182, 'bag': 20, 'out': 184, 'check': 41, 'again': 6, 'ended': 74, 'pair': 186, 'plane': 197, 'wooow': 275, 'crime': 54, 'considering': 48, 'getting': 101, 'dog': 65, 'thoughts': 249, 'cat': 37, 'cute': 58, 'cats': 38, 'paws': 190, 'addorable': 5, 'allergic': 10, 'though': 247, 'especially': 78, 'hair': 110, 'balls': 22, 'understand': 262, 'forbidden': 94, 'tweezers': 261, 'kidding': 133, 'need': 165, 'least': 140, '50': 0, 'messages': 158, 'pretty': 204, 'picture': 194, 'she': 227, 'exactly': 83, 'payback': 191, 'babysitting': 19, 'made': 150, 'pension': 192, 'times': 253, 'abroad': 3, 'eilat': 71, 'old': 178, 'favorite': 87, 'meal': 155, 'sorry': 231, 'personal': 193, 'question': 209, '67': 1, 'problem': 205, 'love': 148, 'italian': 131, 'food': 93, 'carbs': 36, 'burgers': 32, 'yum': 286, 'arenchini': 15, 'don': 66, 'say': 219, 'english': 76, 'entrecote': 77, 'hungry': 121, 'men': 156, 'cuisine': 55, 'popular': 202, 'influential': 124, 'culinary': 56, 'traditions': 257, 'known': 136, 'emphasis': 73, 'fresh': 97, 'high': 116, 'quality': 208, 'ingredients': 127, 'regional': 212, 'diversity': 64, 'balance': 21, 'simplicity': 229, 'flavor': 92, 'evolved': 82, 'centuries': 39, 'influenced': 123, 'various': 264, 'factors': 85, 'geography': 100, 'economic': 68, 'conditions': 46, 'neighboring': 167, 'cultures': 57, 'ate': 16, 'yummy': 287, 'hours': 120, 'ago': 7, 'thought': 248, 'beginning': 24, 'wrote': 280, 'crabs': 52, 'kosher': 138, 'but': 33, 'delicious': 61, 'pardon': 188, 'religious': 214, 'data': 59, 'looks': 147, 'export': 84, 'class': 44, 'thank': 241, 'cooperation': 51}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "QT4GQ4PDeiyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import time\n",
        "\n",
        "def calculate_time_complexity(texts):\n",
        "    complexities = {}\n",
        "    # Initialize vectorizers\n",
        "    vectorizers = [\n",
        "        ('word', TfidfVectorizer(max_features=500)),\n",
        "        ('char', TfidfVectorizer(analyzer='char')),\n",
        "        ('char_wb', TfidfVectorizer(analyzer='char_wb'))]\n",
        "\n",
        "    # Iterate over each vectorizer\n",
        "    for name, vectorizer in vectorizers:\n",
        "        start_time = time.time()\n",
        "        # Fit and transform the text data\n",
        "        features = vectorizer.fit_transform(texts)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Calculate the time taken for fit_transform\n",
        "        time_taken = end_time - start_time\n",
        "\n",
        "        # Get the vocabulary size\n",
        "        vocab_size = len(vectorizer.get_feature_names_out())\n",
        "\n",
        "        # Store the complexity\n",
        "        complexities[name] = {'time_taken': time_taken, 'vocab_size': vocab_size}\n",
        "\n",
        "    return complexities, features"
      ],
      "metadata": {
        "id": "3hNFLf_Nek4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complexities, features_tfidf = calculate_time_complexity(filtered_text_flat)\n",
        "\n",
        "# Print the results\n",
        "for name, complexity in complexities.items():\n",
        "    print(f\"{name} vectorizer:\")\n",
        "    print(f\"Time taken: {complexity['time_taken']:.6f} seconds\")\n",
        "    print(f\"Vocabulary size: {complexity['vocab_size']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBoHSO_jjzYr",
        "outputId": "b399a41e-3bd6-482a-e31c-c389a102d20c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word vectorizer:\n",
            "Time taken: 0.007707 seconds\n",
            "Vocabulary size: 288\n",
            "\n",
            "char vectorizer:\n",
            "Time taken: 0.007244 seconds\n",
            "Vocabulary size: 75\n",
            "\n",
            "char_wb vectorizer:\n",
            "Time taken: 0.005818 seconds\n",
            "Vocabulary size: 76\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "times = [complexity['time_taken'] for complexity in complexities.values()]\n",
        "vocab_sizes = [complexity['vocab_size'] for complexity in complexities.values()]\n",
        "\n",
        "# Calculate the time complexity\n",
        "time_complexity = sum(time * vocab_size for time, vocab_size in zip(times, vocab_sizes))\n",
        "\n",
        "print(\"Time complexity:\", time_complexity, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7j2vBX7pXLH",
        "outputId": "5063809c-4052-43f4-8b92-a7c28217436f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time complexity: 3.205172300338745 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "nSlP5IwKela6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def train_word2vec(texts):\n",
        "    model = Word2Vec(sentences=clean_stop_words(lines_tokens), vector_size=100, window=5, min_count=1, workers=4)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-CqUzrZwenbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = train_word2vec(clean_stop_words(lines_tokens))"
      ],
      "metadata": {
        "id": "mWTsgGlhrCnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in filtered_text_flat:\n",
        "    word_embedding = word2vec_model.wv[word]\n",
        "    #print(\"Word:\", word)\n",
        "    #print(\"Word embedding vector:\", word_embedding)"
      ],
      "metadata": {
        "id": "xW0kAGJmr_VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_word2vec = [word2vec_model.wv[word] for word in filtered_text_flat if word in word2vec_model.wv]"
      ],
      "metadata": {
        "id": "sy1o5zQ1XGmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(word2vec_model.wv.key_to_index)\n",
        "embedding_size = word2vec_model.vector_size\n",
        "size_of_word_vector_data_type = 4  # bytes\n",
        "space_complexity = vocabulary_size * embedding_size * size_of_word_vector_data_type\n",
        "\n",
        "print(\"Space complexity of Word2Vec model:\", space_complexity, \"bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozl_kSKaveVc",
        "outputId": "427bb6a3-59a0-470f-d6dc-b4460b5d2026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Space complexity of Word2Vec model: 137200 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe"
      ],
      "metadata": {
        "id": "2D2U_7xqeoLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Function to load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file):\n",
        "    word_vectors = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                line_parts = line.strip().split()\n",
        "                word = line_parts[0]\n",
        "                vector = [float(val) for val in line_parts[1:]]\n",
        "                word_vectors[word] = vector\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing line: {line}\")\n",
        "                print(e)\n",
        "    return word_vectors\n",
        "\n",
        "# URL of the GloVe embeddings zip file\n",
        "url = \"https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip\"\n",
        "\n",
        "# Download the zip file\n",
        "response = requests.get(url)\n",
        "with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")  # Specify the directory where you want to extract the files"
      ],
      "metadata": {
        "id": "oBuiOfD_HT8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load GloVe embeddings\n",
        "glove_file = \"/content/glove.6B.100d.txt\"\n",
        "word_vectors = load_glove_embeddings(glove_file)"
      ],
      "metadata": {
        "id": "OiZslV49H431"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract features using GloVe embeddings\n",
        "def GloVe(sentence, word_vectors):\n",
        "    words = sentence.split()\n",
        "    features = []\n",
        "    for word in words:\n",
        "        if word in word_vectors:\n",
        "            features.append(word_vectors[word])\n",
        "    return features"
      ],
      "metadata": {
        "id": "aNJ9v8_qEiL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the list of words into a single string\n",
        "input_sentence = ' '.join(filtered_text_flat)\n",
        "# Extract features for the example sentence\n",
        "features_glove = GloVe(input_sentence, word_vectors)\n",
        "\n",
        "# Now 'features' contains the GloVe embeddings for the words in the sentence\n",
        "print(features_glove)"
      ],
      "metadata": {
        "id": "wRYgVWu5Eg9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_unique_words = 0\n",
        "dimensionality = None\n",
        "\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        num_unique_words += 1\n",
        "        if dimensionality is None:\n",
        "            dimensionality = len(line.strip().split()[1:])\n",
        "\n",
        "# Calculate space complexity for GloVe embeddings\n",
        "space_complexity_glove = num_unique_words * dimensionality\n",
        "\n",
        "# Calculate space complexity for input sentence\n",
        "input_sentence = ' '.join(filtered_text_flat)\n",
        "space_complexity_input_sentence = len(input_sentence)\n",
        "\n",
        "# Calculate space complexity for features\n",
        "num_words_input_sentence = len(input_sentence.split())\n",
        "space_complexity_features = num_words_input_sentence * dimensionality\n",
        "\n",
        "# Calculate total space complexity\n",
        "total_space_complexity = space_complexity_glove + space_complexity_input_sentence + space_complexity_features\n",
        "\n",
        "print(\"Space complexity for GloVe embeddings:\", space_complexity_glove)\n",
        "print(\"Space complexity for input sentence:\", space_complexity_input_sentence)\n",
        "print(\"Space complexity for features:\", space_complexity_features)\n",
        "print(\"Total space complexity:\", total_space_complexity)"
      ],
      "metadata": {
        "id": "Szu7FhRneNRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare most frequent features"
      ],
      "metadata": {
        "id": "DVRV8AUgiYr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "features_bow\n",
        "features_tfidf\n",
        "features_word2vec\n",
        "features_glove\n",
        "\n",
        "# Count frequencies of features for each method\n",
        "freq_bow = Counter(features_bow.toarray().flatten())\n",
        "freq_tfidf = Counter(features_tfidf.toarray().flatten().tolist())\n",
        "freq_word2vec = Counter(tuple(feature) for feature in features_word2vec)\n",
        "freq_glove = Counter(tuple(feature) for feature in features_glove)\n",
        "\n",
        "# Find most frequent features\n",
        "top_features_bow = freq_bow.most_common(10)\n",
        "top_features_tfidf = freq_tfidf.most_common(10)\n",
        "top_features_word2vec = freq_word2vec.most_common(10)\n",
        "top_features_glove = freq_glove.most_common(10)\n",
        "\n",
        "methods = [\"Bag-of-Words\", \"TF-IDF\", \"Word2Vec\", \"GloVe\"]\n",
        "top_features = [top_features_bow, top_features_tfidf, top_features_word2vec, top_features_glove]\n",
        "\n",
        "# Print most frequent features for each method\n",
        "for method, features in zip(methods, top_features):\n",
        "    print(f\"Most frequent features extracted using {method}:\")\n",
        "    for feature, count in features:\n",
        "        print(count, \"-\", feature)\n",
        "    print()"
      ],
      "metadata": {
        "id": "LzkaXoDreQQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS tagging using HMM with NLTK"
      ],
      "metadata": {
        "id": "rJ1H63IjbxQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices = [63, 64, 66, 67, 68]\n",
        "five_sentences = [cleaned_lines[i] for i in indices]"
      ],
      "metadata": {
        "id": "x6cS-vT3aGdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(five_sentences)):\n",
        "    print(i, five_sentences[i])"
      ],
      "metadata": {
        "id": "Yi-g06Cne63u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('treebank')\n",
        "from nltk.tag import hmm\n",
        "from nltk.corpus import treebank"
      ],
      "metadata": {
        "id": "IhEWxXd3uJwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Penn Treebank corpus for training\n",
        "tagged_sentences = treebank.tagged_sents()\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in five_sentences]\n",
        "\n",
        "# Train the HMM POS tagger\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train_supervised(tagged_sentences)\n",
        "\n",
        "# Tag each token in the sentences\n",
        "tagged_sentences = [tagger.tag(sentence) for sentence in tokenized_sentences]\n",
        "\n",
        "# Print the tagged sentences\n",
        "for i, tagged_sentence in enumerate(tagged_sentences, start=1):\n",
        "    print(f\"Sentence {i}:\")\n",
        "    print(tagged_sentence)\n",
        "    print()"
      ],
      "metadata": {
        "id": "v9yGCLJtfU4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS tagging using CYK algorithm manually"
      ],
      "metadata": {
        "id": "0Zo0yZrpb23N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example context-free grammar (CFG) with simple POS rules\n",
        "grammar = {\n",
        "    'S': [['NP', 'VP']],\n",
        "    'NP': [['Det', 'N'], ['Pronoun']],\n",
        "    'VP': [['V'], ['V', 'NP'], ['V', 'NP', 'Adverb']],\n",
        "    'Det': ['I', 'a', 'the'],\n",
        "    'N': ['dog', 'cat', 'cats' 'thoughts', 'paws'],\n",
        "    'Pronoun': ['I', 'Their'],\n",
        "    'V': ['considering', 'getting', 'take', 'like', 'are', 'allergic'],\n",
        "    'Adverb': ['though','cute','addorable'],\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "c8MazLrEuxqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_pos_tags(sentence, parse_table):\n",
        "    n = len(sentence)\n",
        "    pos_tags = [''] * n\n",
        "    for length in range(1, n + 1):\n",
        "        for start in range(n - length + 1):\n",
        "            end = start + length - 1\n",
        "            tags = parse_table[start][end]\n",
        "            if len(tags) == 1:\n",
        "                pos_tags[start] = list(tags)[0]\n",
        "    return pos_tags"
      ],
      "metadata": {
        "id": "lxkjzjXzONr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-terminal symbols\n",
        "non_terminals = [\"NP\", \"Nom\", \"Det\", \"AP\",\n",
        "                  \"Adv\", \"A\"]\n",
        "terminals = [\"considering\", \"getting\", \"thoughts\",\n",
        "             \"cat\", \"cats\",\n",
        "             \"paws\", \"allergic\"]\n",
        "\n",
        "\n",
        "\n",
        "# Function to perform the CYK Algorithm\n",
        "def cykParse(w):\n",
        "    n = len(w)\n",
        "\n",
        "    # Initialize the table\n",
        "    T = [[set([]) for j in range(n)] for i in range(n)]\n",
        "\n",
        "    # Filling in the table\n",
        "    for j in range(0, n):\n",
        "\n",
        "        # Iterate over the rules\n",
        "        for lhs, rule in R.items():\n",
        "            for rhs in rule:\n",
        "\n",
        "                # If a terminal is found\n",
        "                if len(rhs) == 1 and \\\n",
        "                rhs[0] == w[j]:\n",
        "                    T[j][j].add(lhs)\n",
        "\n",
        "        for i in range(j, -1, -1):\n",
        "\n",
        "            # Iterate over the range i to j + 1\n",
        "            for k in range(i, j + 1):\n",
        "\n",
        "                # Iterate over the rules\n",
        "                for lhs, rule in R.items():\n",
        "                    for rhs in rule:\n",
        "\n",
        "                        # If a terminal is found\n",
        "                        if len(rhs) == 2:\n",
        "                          if rhs[0] in T[i][k]:\n",
        "                            if rhs[1] in T[k + 1][j]:\n",
        "                                T[i][j].add(lhs)\n",
        "\n",
        "    return T"
      ],
      "metadata": {
        "id": "SaVgesKbeL2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max(len(sentence) for sentence in tokenized_sentences)\n",
        "print('Max length sentence: ', max_length)\n",
        "padded_tokenized_sentences = [sentence + ['<PAD>'] * (max_length - len(sentence)) for sentence in tokenized_sentences]\n",
        "print(padded_tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opPYKkdLje9f",
        "outputId": "2ccc139c-4dff-4931-a121-5b3ba2512d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length sentence:  9\n",
            "[['I', \"'m\", 'considering', 'getting', 'a', 'dog', ',', 'thoughts', '?'], ['take', 'a', 'cat', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['Cute', '!', 'i', 'like', 'cats', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['Their', 'paws', 'are', 'addorable', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['I', \"'m\", 'allergic', 'though', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rules of the grammar\n",
        "R = {\n",
        "     \"NP\": [[\"Det\", \"Nom\"]],\n",
        "     \"Nom\": [[\"AP\", \"Nom\"], [\"dog\"],\n",
        "             [\"cat\"], [\"cats\"],['paws']],\n",
        "     \"AP\": [[\"Adv\", \"A\"], [\"considering \"]\n",
        "            ,[\"take \"], [\"addorable\"],['allergic'],[\"like\"],[\"though\"]],\n",
        "     \"Det\": [[\"a\"],[\"'m\"],[\"a\"],[\"are\"]],\n",
        "     \"Adv\": [[\"Cute\"], [\"like\"],[\"considering\"],[\"thoughts\"]],\n",
        "     \"A\": [[\"I\"], [\"allergic \"], [\"thoughts \"],\n",
        "           [\"take\"],[\"getting\"],[\",\"],[\"!\"],[\"i\"]]\n",
        "    }"
      ],
      "metadata": {
        "id": "lYFqDmsqmHTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in padded_tokenized_sentences:\n",
        "    parse_table = cykParse(sentence)\n",
        "    pos_tags = assign_pos_tags(sentence, parse_table)\n",
        "    print(\"Sentence:\", \" \".join(sentence))\n",
        "    print(\"POS tags:\", pos_tags)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPHhNkyEallp",
        "outputId": "49b36151-4541-4477-8b86-a0e351270e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: I 'm considering getting a dog , thoughts ?\n",
            "POS tags: ['A', 'Det', 'AP', 'A', 'NP', 'Nom', 'A', 'Adv', '']\n",
            "\n",
            "Sentence: take a cat <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "POS tags: ['A', 'NP', 'Nom', '', '', '', '', '', '']\n",
            "\n",
            "Sentence: Cute ! i like cats <PAD> <PAD> <PAD> <PAD>\n",
            "POS tags: ['AP', 'A', 'A', 'Nom', 'Nom', '', '', '', '']\n",
            "\n",
            "Sentence: Their paws are addorable <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "POS tags: ['', 'Nom', 'Det', 'AP', '', '', '', '', '']\n",
            "\n",
            "Sentence: I 'm allergic though <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "POS tags: ['A', 'Det', 'AP', 'AP', '', '', '', '', '']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Tagging"
      ],
      "metadata": {
        "id": "gZwau9duPgjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "total_sentences = len(tagged_sentences)\n",
        "correct_sentences = 0\n",
        "\n",
        "for tagged_sentence, gold_pos_tags in zip(tagged_sentences, pos_tags):\n",
        "    # Check if lengths are the same\n",
        "    if len(tagged_sentence) != len(gold_pos_tags):\n",
        "        continue\n",
        "\n",
        "    # Check if tags match\n",
        "    correct_tags = all(tagged_token[1] == gold_tag for tagged_token, gold_tag in zip(tagged_sentence, gold_pos_tags))\n",
        "    if correct_tags:\n",
        "        correct_sentences += 1\n",
        "\n",
        "accuracy = correct_sentences / total_sentences * 100\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Clk_p3_XLPoL",
        "outputId": "156bf0d7-fb8e-42ac-ea34-d0c1f3047898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse using NLTK's Chart Parser"
      ],
      "metadata": {
        "id": "Jk1GV_jKPuc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chart_parser = nltk.ChartParser(grammar, trace=5)"
      ],
      "metadata": {
        "id": "cVM1DSyaPocf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(tokenized_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9ncZ0z9zsIc",
        "outputId": "b31cdf40-6417-420e-90ee-da245a790a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_to_drop = [\"'m\", '?']\n",
        "filtered_sentences = [[word for word in sentence if word not in tokens_to_drop] for sentence in tokenized_sentences]"
      ],
      "metadata": {
        "id": "peOPHFmQ07Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67ay6O_A1IBL",
        "outputId": "49347918-e077-4d59-fd1c-3188b27a9b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['I', 'considering', 'getting', 'a', 'dog', ',', 'thoughts'],\n",
              " ['take', 'a', 'cat'],\n",
              " ['Cute', '!', 'i', 'like', 'cats'],\n",
              " ['Their', 'paws', 'are', 'addorable'],\n",
              " ['I', 'allergic', 'though']]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the chart parser\n",
        "grammar_string = \"\"\"\n",
        "NP -> Det Nom\n",
        "Nom -> AP Nom | 'dog' | 'cat' | 'cats' | 'paws'\n",
        "AP -> Adv A | 'considering' | 'take' | 'adorable' | 'allergic' | 'like' | 'though'\n",
        "Det -> 'a' | 'the'\n",
        "Adv -> 'Cute' | 'like' | 'considering' | 'adorable' | 'Their'| 'addorable'\n",
        "A -> 'I' | 'are' | 'thoughts' | 'take' | 'getting' | ',' | '!' | 'i'\n",
        "\"\"\"\n",
        "\n",
        "grammarToParse = nltk.CFG.fromstring(grammar_string)\n",
        "chart_parser = nltk.ChartParser(grammarToParse, trace=5)\n",
        "\n",
        "for sentence in filtered_sentences:\n",
        "    print(\"Sentence:\", \" \".join(sentence))\n",
        "    for tree in chart_parser.parse(sentence):\n",
        "        print(tree)"
      ],
      "metadata": {
        "id": "chztl7fmP-fk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0738807e-c03d-4c32-f991-2ed1d6cef0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: I considering getting a dog , thoughts\n",
            "|.  I  .consi.getti.  a  . dog .  ,  .thoug.|\n",
            "Leaf Init Rule:\n",
            "|[-----]     .     .     .     .     .     .| [0:1] 'I'\n",
            "|.     [-----]     .     .     .     .     .| [1:2] 'considering'\n",
            "|.     .     [-----]     .     .     .     .| [2:3] 'getting'\n",
            "|.     .     .     [-----]     .     .     .| [3:4] 'a'\n",
            "|.     .     .     .     [-----]     .     .| [4:5] 'dog'\n",
            "|.     .     .     .     .     [-----]     .| [5:6] ','\n",
            "|.     .     .     .     .     .     [-----]| [6:7] 'thoughts'\n",
            "Bottom Up Predict Combine Rule:\n",
            "|[-----]     .     .     .     .     .     .| [0:1] A  -> 'I' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.     [-----]     .     .     .     .     .| [1:2] AP -> 'considering' *\n",
            "|.     [-----]     .     .     .     .     .| [1:2] Adv -> 'considering' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.     [----->     .     .     .     .     .| [1:2] AP -> Adv * A\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.     [----->     .     .     .     .     .| [1:2] Nom -> AP * Nom\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.     .     [-----]     .     .     .     .| [2:3] A  -> 'getting' *\n",
            "Single Edge Fundamental Rule:\n",
            "|.     [-----------]     .     .     .     .| [1:3] AP -> Adv A *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.     [----------->     .     .     .     .| [1:3] Nom -> AP * Nom\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.     .     .     [-----]     .     .     .| [3:4] Det -> 'a' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.     .     .     [----->     .     .     .| [3:4] NP -> Det * Nom\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.     .     .     .     [-----]     .     .| [4:5] Nom -> 'dog' *\n",
            "Single Edge Fundamental Rule:\n",
            "|.     .     .     [-----------]     .     .| [3:5] NP -> Det Nom *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.     .     .     .     .     [-----]     .| [5:6] A  -> ',' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.     .     .     .     .     .     [-----]| [6:7] A  -> 'thoughts' *\n",
            "Sentence: take a cat\n",
            "|.    take   .     a     .    cat    .|\n",
            "Leaf Init Rule:\n",
            "|[-----------]           .           .| [0:1] 'take'\n",
            "|.           [-----------]           .| [1:2] 'a'\n",
            "|.           .           [-----------]| [2:3] 'cat'\n",
            "Bottom Up Predict Combine Rule:\n",
            "|[-----------]           .           .| [0:1] AP -> 'take' *\n",
            "|[-----------]           .           .| [0:1] A  -> 'take' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|[----------->           .           .| [0:1] Nom -> AP * Nom\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.           [-----------]           .| [1:2] Det -> 'a' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.           [----------->           .| [1:2] NP -> Det * Nom\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.           .           [-----------]| [2:3] Nom -> 'cat' *\n",
            "Single Edge Fundamental Rule:\n",
            "|.           [-----------------------]| [1:3] NP -> Det Nom *\n",
            "Sentence: Cute ! i like cats\n",
            "|.  Cute .   !   .   i   .  like .  cats .|\n",
            "Leaf Init Rule:\n",
            "|[-------]       .       .       .       .| [0:1] 'Cute'\n",
            "|.       [-------]       .       .       .| [1:2] '!'\n",
            "|.       .       [-------]       .       .| [2:3] 'i'\n",
            "|.       .       .       [-------]       .| [3:4] 'like'\n",
            "|.       .       .       .       [-------]| [4:5] 'cats'\n",
            "Bottom Up Predict Combine Rule:\n",
            "|[-------]       .       .       .       .| [0:1] Adv -> 'Cute' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|[------->       .       .       .       .| [0:1] AP -> Adv * A\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.       [-------]       .       .       .| [1:2] A  -> '!' *\n",
            "Single Edge Fundamental Rule:\n",
            "|[---------------]       .       .       .| [0:2] AP -> Adv A *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|[--------------->       .       .       .| [0:2] Nom -> AP * Nom\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.       .       [-------]       .       .| [2:3] A  -> 'i' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.       .       .       [-------]       .| [3:4] AP -> 'like' *\n",
            "|.       .       .       [-------]       .| [3:4] Adv -> 'like' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.       .       .       [------->       .| [3:4] AP -> Adv * A\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.       .       .       [------->       .| [3:4] Nom -> AP * Nom\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.       .       .       .       [-------]| [4:5] Nom -> 'cats' *\n",
            "Single Edge Fundamental Rule:\n",
            "|.       .       .       [---------------]| [3:5] Nom -> AP Nom *\n",
            "Sentence: Their paws are addorable\n",
            "|.  Their  .   paws  .   are   .addorable.|\n",
            "Leaf Init Rule:\n",
            "|[---------]         .         .         .| [0:1] 'Their'\n",
            "|.         [---------]         .         .| [1:2] 'paws'\n",
            "|.         .         [---------]         .| [2:3] 'are'\n",
            "|.         .         .         [---------]| [3:4] 'addorable'\n",
            "Bottom Up Predict Combine Rule:\n",
            "|[---------]         .         .         .| [0:1] Adv -> 'Their' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|[--------->         .         .         .| [0:1] AP -> Adv * A\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.         [---------]         .         .| [1:2] Nom -> 'paws' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.         .         [---------]         .| [2:3] A  -> 'are' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.         .         .         [---------]| [3:4] Adv -> 'addorable' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.         .         .         [--------->| [3:4] AP -> Adv * A\n",
            "Sentence: I allergic though\n",
            "|.     I     .  allergic .   though  .|\n",
            "Leaf Init Rule:\n",
            "|[-----------]           .           .| [0:1] 'I'\n",
            "|.           [-----------]           .| [1:2] 'allergic'\n",
            "|.           .           [-----------]| [2:3] 'though'\n",
            "Bottom Up Predict Combine Rule:\n",
            "|[-----------]           .           .| [0:1] A  -> 'I' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.           [-----------]           .| [1:2] AP -> 'allergic' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.           [----------->           .| [1:2] Nom -> AP * Nom\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.           .           [-----------]| [2:3] AP -> 'though' *\n",
            "Bottom Up Predict Combine Rule:\n",
            "|.           .           [----------->| [2:3] Nom -> AP * Nom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theory\n"
      ],
      "metadata": {
        "id": "hdL3vk0cBSsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To illustrate the impact of input data on Word2Vec algorithm output, let's consider two text examples from the healthcare domain, specifically focusing on the topic of a healthy lifestyle. These examples will be constructed to share high semantic similarity but are articulated in significantly different styles, leading to potentially different Word2Vec embeddings.\n",
        "\n",
        "*Text Example 1*: Medical Article on Healthy Living\n",
        "\"Adopting a healthy lifestyle is pivotal for maintaining optimal health and preventing chronic diseases. A balanced diet, rich in fruits, vegetables, whole grains, and lean proteins, provides essential nutrients and energy. Regular physical activity, comprising at least 150 minutes of moderate aerobic exercise weekly, enhances cardiovascular health and supports weight management. Additionally, adequate sleep, stress management, and abstaining from harmful habits like smoking and excessive alcohol consumption contribute to overall well-being. These practices are endorsed by health professionals worldwide as foundations for a healthy life.\"\n",
        "\n",
        "*Text Example 2*: Personal Blog Post on Staying Healthy\n",
        "\"Staying healthy isn‚Äôt just about hitting the gym or munching on salads all day. It‚Äôs a mix of things - eating stuff that‚Äôs good for you (think more greens and proteins), getting your body moving, cutting down on the booze and cigarettes, and trying not to stress out too much. Oh, and don‚Äôt forget to catch enough Zs! It‚Äôs pretty much what every fitness guru on the internet preaches, but hey, they‚Äôre not wrong. Living healthy can keep those nasty illnesses at bay and just make you feel better overall.\"\n",
        "\n",
        "*Characteristics and Human Perception of Similarity*\n",
        "Both texts emphasize the importance of a holistic approach to health, encompassing diet, exercise, sleep, and avoidance of negative behaviors. Humans would perceive them as similar because they both advocate for a comprehensive health strategy, despite the difference in presentation.\n",
        "\n",
        "*Impact on Word2Vec Embeddings*\n",
        "Vocabulary and Formality: The medical article uses formal and technical language (\"optimal health,\" \"chronic diseases,\" \"moderate aerobic exercise\"), while the blog post adopts a conversational tone with colloquial expressions (\"hitting the gym,\" \"munching on salads,\" \"catch enough Zs\"). This variance influences Word2Vec's context-based learning, likely generating different embeddings for the same concepts expressed in each text.\n",
        "\n",
        "*Context and Syntax*: The structured exposition in the medical article contrasts with the narrative, almost anecdotal style of the blog post. This difference not only affects the syntax but also how concepts are contextualized, impacting the learned embeddings as Word2Vec interprets the semantic significance of words based on their surrounding words.\n",
        "\n",
        "*Conceptual Representation*: Despite discussing the same health practices, the framing in each text (e.g., \"preventing chronic diseases\" vs. \"keep those nasty illnesses at bay\") presents challenges for Word2Vec in recognizing semantic equivalences across diverse linguistic contexts.\n",
        "\n",
        "*Why Word2Vec Might Fail to Capture Semantic Similarity*\n",
        "Word2Vec's mechanism, which relies on predicting a word from its context or vice versa, might not fully capture the semantic similarity when the expressions of similar ideas are contextualized differently. The algorithm's focus on local context means that even when two texts share a thematic core, variations in diction, style, and structure can lead to divergent embeddings. Essentially, Word2Vec might miss the forest for the trees, getting caught up in the specifics of expression rather than the underlying similarity in content.\n",
        "\n",
        "*Influence of Input Data Quality and Representativeness*\n",
        "The quality and diversity of the training corpus significantly influence Word2Vec's performance. If the model is predominantly exposed to formal medical texts, its embeddings for colloquial expressions might be underdeveloped. Conversely, a model trained mainly on informal narratives may struggle with technical terminology, affecting its ability to generalize and recognize semantic similarities across different styles.\n",
        "\n",
        "*Improvements for Word2Vec*\n",
        "Enhanced Contextual Awareness: Incorporating broader contextual information or global document context could improve semantic understanding, enabling Word2Vec to better grasp the overall message beyond local word patterns.\n",
        "\n",
        "*Cross-Style Training*: Ensuring the training dataset includes a wide variety of linguistic styles, from formal to informal, can help develop a more nuanced model that recognizes semantic similarities across different expressions.\n",
        "\n",
        "*Integrating Syntactic Information*: Augmenting Word2Vec with models that consider syntactic relationships, such as dependency parsing information, might help bridge the gap between different ways of expressing similar ideas.\n",
        "\n",
        "Without specific visualization of Word2Vec outputs here, one could use techniques like t-SNE or PCA for dimensionality reduction to visually compare embeddings from the two texts, highlighting how similar concepts (e.g., \"balanced diet\" and \"eating stuff that‚Äôs good for you\") diverge in embedding space due to differences in their linguistic presentation. This discussion underscores the nuanced impact of input variation on embeddings and points towards avenues for enhancing Word2Vec's capacity to recognize semantic similarity across diverse textual expressions."
      ],
      "metadata": {
        "id": "lQRSUuUuCUgy"
      }
    }
  ]
}